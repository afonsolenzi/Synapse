{
	"name": "D3_SQLOD_ETL_Tables",
	"properties": {
		"folder": {
			"name": "Music Service"
		},
		"content": {
			"query": "-- AFTER running the D2 - D3 Pipelines\n-- test the contents of the external table\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://warnerdatalake.dfs.core.windows.net/synapse/musicdata/augmentedArtistInfo.parquet',\n        FORMAT='PARQUET'\n    ) AS [result];\n\n-- drop external table  [dbo].[augmentedArtistInformation]\nCREATE EXTERNAL TABLE [dbo].[augmentedArtistInformation]\n(\n\t[ArtistName] [varchar](100),\n\t[Rate] [int],\n\t[ticketSalesDollars] [int]\n)\nWITH (DATA_SOURCE = synapse,LOCATION = N'/musicdata/augmentedArtistInfo.parquet',FILE_FORMAT = [ParquetFF]);\nGO\n\nSELECT TOP 10 * FROM dbo.augmentedArtistInformation;\n-- END OF D2 - D3\n\n-- DEACTIVATE THE D1 TRIGGER AND PUBLISH BEFORE DOING D5!!\n-- after running the D5_MDF_UserArtistPlays_Contained pipeline\nCREATE EXTERNAL TABLE [dbo].[userArtistPlaysETL]\n(\n    [UserID] varchar(100),\n\t[Artist] varchar(100),\n\t[Plays] int,\n\t[sourceFile] varchar(300)\n)\nWITH (DATA_SOURCE=synapse,LOCATION = N'/uploads/userartistplays/processed/*.parquet',FILE_FORMAT = [ParquetFF]);\nGO\n\nSELECT count(*) from userArtistPlaysETL\n-- END OF D5\n\n-- For WDF D7 demo, we want to read from the userdetails Spark table \n-- How do we access the spark data from the Serverless pool in the Power Query interface?\n\nselect top 10 * from musicservice.dbo.user_details\n\n-- what if you go through a view?\nCREATE VIEW userdetailsfromspark\nAS\nSELECT * FROM musicservice.dbo.user_details\n\nSELECT top 10 * from userdetailsfromspark; -- works here but won't work as a Power Query input\n\n-- unfortunately this doesn't work for a WDF because the SQL Authentication\n-- is not able to access the Spark folder underneath (because it lacks the AAD token authorization).\n\n-- Wrangling Data Flows require SQL Authentication\n-- and SQL Authentication can only access the data through a SAS token in an External table definition\n-- link: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-storage-access-control\nCREATE DATABASE SCOPED CREDENTIAL [sqlondemandsas]\nWITH IDENTITY='SHARED ACCESS SIGNATURE',  \nSECRET = 'sv=2019-12-12&ss=b&srt=co&sp=rwdlacx&se=2030-11-25T04:15:57Z&st=2020-11-23T20:15:57Z&spr=https&sig=Hgs3djfSkluaxN6%2Bv6dYgzz6XRbUSuiz3q%2BrG3Sh11Y%3D'  --fill your storage SAS here\n\nCREATE EXTERNAL DATA SOURCE synapsesas\nWITH (    LOCATION   = 'https://warnerdatalake.dfs.core.windows.net/synapse',\n          CREDENTIAL = [sqlondemandsas] \n)\nGO\n\nCREATE EXTERNAL TABLE userdetails\n(\n    UserID varchar(50),\n    Gender varchar(2),\n    Age int,\n    Country varchar(50),\n    SignupDate date\n)\n WITH (\n        LOCATION = '/synapse/workspaces/warnersynapse/warehouse/musicservice.db/user_details/',\n        DATA_SOURCE = synapsesas,\n        FILE_FORMAT = ParquetFF\n)\n\nselect top 10 * from userdetails\n\n-- AFTER you run pipeline demo D7 then you can look at the result\nCREATE EXTERNAL TABLE userdetailswrangled\n(\n    UserID varchar(50), \n    Gender varchar(10),\n    Age int,\n    AgeRange varchar(10),\n    Country varchar(50),\n    SignupDate date\n)\n WITH (\n        LOCATION = '/musicdata/userdetailswrangled/*',\n        DATA_SOURCE = synapsesas,\n        FILE_FORMAT = ParquetFF\n)\n\nselect top 10 * from userdetailswrangled\n-- END OF D7\n\n-- BEFORE Pipelines Demo D8\n-- now let's look at bringing in time of day timestamps from an on-prem SQL Server\nCREATE EXTERNAL TABLE usersignuplog_watermark WITH (\n        LOCATION = '/musicdata/etl/usersignuplog_watermark/',\n        DATA_SOURCE = synapse,\n        FILE_FORMAT = PipeDelimitedWithNoHeaderFormat\n) AS\nSELECT -2 watermark,getdate() [timestamp];\n\nSELECT top 1 watermark FROM usersignuplog_watermark\norder by [timestamp] desc;\n\n-- this won't work because serverless doesn't do DML, so how do we keep the watermark updated?\n-- Check the pipeline D8!\nINSERT INTO usersignuplog_watermark(watermark, [timestamp])\nVALUES(-1,getdate());\n\n-- after the initial seed happens through the D8 pipeline\n\nCREATE EXTERNAL TABLE userSignupTimestamps\n(\n    UserId varchar(50),\n    signup_date date,\n    signup_time int\n)\n WITH (\n        LOCATION = '/musicdata/etl/usersignuptimestamps/',\n        DATA_SOURCE = synapse,\n        FILE_FORMAT = ParquetFF\n)\n\nSELECT top 100 * from userSignupTimestamps;\n\n-- now let's analyze this signup_time\nselect distinct len(convert(varchar(8),signup_time)) from userSignupTimestamps\n\nselect top 10 * from userSignupTimestamps\nwhere len(convert(varchar(8),signup_time))=1 -- 0 -> 00:00:00\n-- 20 -> 00:00:20\nselect top 10 * from userSignupTimestamps\nwhere len(convert(varchar(8),signup_time))=3 -- 300 -> 00:03:00\n\nselect top 10 * from userSignupTimestamps\nwhere len(convert(varchar(8),signup_time))=4 -- 5500-> 00:55:00\n\nselect top 10 * from userSignupTimestamps\nwhere len(convert(varchar(8),signup_time))=5 -- 91300 -> 09:13:00\n\nselect top 10 * from userSignupTimestamps\nwhere len(convert(varchar(8),signup_time))=6 -- 210500 -> 21:05:00\n\n-- we can wrangle these as well to get a proper time column using WDF!\n-- END OF D8\n\n-- Use Pipelines D9 at this point to do this time conversion (D9 depends on having completed D8!)\n-- After D9_WDF pipelines is done we can create the external table for the user details with the new schema\n-- and then we have the full signup details in the right data types\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK '/musicdata/etl/userdetailswithtime/*parquet',\n        DATA_SOURCE='synapse',\n        FORMAT='PARQUET'\n    ) AS [result];\n\nCREATE EXTERNAL TABLE userDetailsComplete\n(\n    UserID varchar(50), \n    Gender varchar(10),\n    Age int,\n    AgeRange varchar(10),\n    Country varchar(50),\n    SignupDatetime datetime\n)\n WITH (\n        LOCATION = '/musicdata/etl/userdetailswithtime/',\n        DATA_SOURCE = synapse,\n        FILE_FORMAT = ParquetFF\n)\n\nselect top 100 * from userDetailsComplete;\n-- END OF D9",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"poolName": "Built-in",
				"databaseName": "demos"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}